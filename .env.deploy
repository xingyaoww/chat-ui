# Use .env.local to change these variables
# DO NOT EDIT THIS FILE WITH SENSITIVE DATA

MONGODB_URL=mongodb://mintagent:mintagentuiuc@mongo-chat-ui:27017/
MONGODB_DB_NAME=chat-ui
MONGODB_DIRECT_CONNECTION=false

COOKIE_NAME=hf-chat
HF_ACCESS_TOKEN=#hf_<token> from from https://huggingface.co/settings/token
HF_API_ROOT=https://api-inference.huggingface.co/models

# used to activate search with web functionality. disabled if none are defined. choose one of the following:
YDC_API_KEY=#your docs.you.com api key here
SERPER_API_KEY=#your serper.dev api key here
SERPAPI_KEY=#your serpapi key here

# used to handling images in the chat
IMAGE_SERVER_URL=http://image-server:80
IMAGE_SERVER_TOKEN=available
IMAGE_SERVER_DB_NAME=image_database

JUPYTER_API_URL=http://128.174.136.21:8008


# Parameters to enable open id login
OPENID_CONFIG=`{
  "PROVIDER_URL": "",
  "CLIENT_ID": "",
  "CLIENT_SECRET": "",
  "SCOPES": ""
}`

# /!\ legacy openid settings, prefer the config above
OPENID_CLIENT_ID=
OPENID_CLIENT_SECRET=
OPENID_SCOPES="openid profile" # Add "email" for some providers like Google that do not provide preferred_username
OPENID_PROVIDER_URL=https://huggingface.co # for Google, use https://accounts.google.com
OPENID_TOLERANCE=
OPENID_RESOURCE=

# Parameters to enable a global mTLS context for client fetch requests
USE_CLIENT_CERTIFICATE=false
CERT_PATH=#
KEY_PATH=#
CA_PATH=#
CLIENT_KEY_PASSWORD=#
REJECT_UNAUTHORIZED=true



# 'name', 'userMessageToken', 'assistantMessageToken' are required
MODELS=`[
  {   
      "endpoints": [
        {
          "url": "http://cs-xing6-01.cs.illinois.edu:8080"
        }
      ],
      "name": "MINT-Agent",
      "userMessageToken": "<|im_start|>user\n",
      "userMessageEndToken": "<|im_end|>",
      "assistantMessageToken": "<|im_start|>assistant\n",
      "assistantMessageEndToken": "<|im_end|>",
      "preprompt": "<|im_start|>system\nYou are a helpful assistant.\nYou can answer questions in natural language, display images through <image> tags (e.g., <image> [image_id] </image>), and execute Python code through <execute> tags (e.g., <execute> print(“hello world”) </execute>). Do this in sequence. Only interpret the results after EXECUTION OUTPUT. Otherwise, \n just generate the code. If there are multiple images at the same time, execute the tool for each function and put the results in a list.  For instance, [explain_concept(image_id_1), explain_concept(image_id_2, 'coyote'). Only for grounding attributes, execute each image in a different <execute> tag. \n\n You have access to the following tools (pre-imported Python functions):\n [1] what_is_object(image_id: str) -> dict\nDescription: Identifies the primary subject or feature in the image. It can be used to answer questions like 'What is this?', 'What is inside this image?' \nExample: <execute>what_is_object('12345abcde')</execute>\n[2] explain_concept(image_id: str) -> dict\nDescription: Explains the context or reason behind the primary subject or feature in the image. It can be used to answer questions like 'Why this is an [object name]?’, ‘Why did you make such predictions?’\nInput Variables: image_id: The unique identifier of the image.\nExample: <execute>explain_concept('12345abcde')</execute>\n\n[4] add_attributes(image_id: str, classname: str, attributes: list) -> dict\nDescription: Adds specified attributes to a given class (concept) in the image.\nInput Variables: image_id: The unique identifier of the image; classname: The class; attributes: Attributes to be added.\nExample: <execute>add_attributes('12345abcde', 'dog', ['brown', 'small'])</execute>\n\n[5] remove_attributes(image_id: str, classname: str, attributes: list) -> dict\nDescription: Removes specified attributes from a given class in the image.\nInput Variables: image_id: The unique identifier of the image; classname: The class; attributes: Attributes to be removed.\nExample: <execute>remove_attributes('12345abcde', 'car', ['red', 'rusty'])</execute>\n\n[6] add_new_concept(image_id: str, correct_concept: str, wrong_concept: str) -> dict\n Description: Replace an old class with a new class in the image. Use this when the user say something like 'no this is not a [wrong_concept] but this is a [correct_concept]', 'This is the [correct_concept].' 'No, this is a [correct_concept]'. \nInput Variables: image_id: The unique identifier of the image; correct_concept: New class input; wrong_concept: Old class to be replaced.\n Example: <execute>old_concept = “Gas Car”\nnew_concept= “Electric Car”\nadd_new_concept('12345abcde', new_concept, old_concept)</execute>\n \n[7] find_activity(image_id: str) -> dict\nDescription: Find the activity inside the image. Use this when user say something like 'What is the activity inside the image?', 'What is the action inside the image?', 'What is the activity inside the image?'. If you need the rationality behind the decision, grounding the object in the image to show the rationality.\nInput Variables: image_id: The unique identifier of the image.\nExample: <execute> find_activity('12345abcde')</execute> \n\n[8] grounding_object_in_image(image_id: str, description: str, ground_tokens: str) -> dict\nDescription: This function identifies and marks specific objects in an image based on the provided description. User can ask something like 'Can you get the ball inside the image?', 'Can you ground the bird inside the image?', 'Can you find the dog inside the image?'.\nInput Variables: image_id: The unique identifier of the image; description: A textual description of the scene or objects in the image. ground_tokens: A comma-separated list of objects to be identified and marked in the image.\nExample: <execute> \ndescription='a dog has a long tail, blue colar'\nground_tokens= 'blue colar'\ngrounding_object_in_image('12345abcde', description, ground_tokens)\n</execute>\n \n[9] get_attributes(classname: str) -> dict\nDescription: Retrieves attributes associated with a given classname.\nInput Variables: classname: The name of the class for which attributes are being retrieved.\nExample: <execute>get_attributes('vehicle')</execute> \n \n[10] reset_server() -> dict\nDescription: Reset the backend server.\n<|im_start|>user\n{\n    'id': '657eafdeef3d6763710dffa2/',\n     'url': '/images/657eafdeef3d6763710dffa2'\n}\n</image>\nwhat is this? <|im_end|>\n<|im_start|>assistant\nimage_id = '657eafdeef3d6763710dffa2'\nresponse = what_is_object(image_id)\nprint(response)\n</execute> <|im_end|>\n<|im_start|>user\nExecution Output:\n{'data': 'This is a 'coyote'', 'type': 'ecole-message'}\n \n<|im_end|>\n<|im_start|>assistant\nThere is a 'coyote' inside the image. <|im_end|>\n<|im_start|>user\nNo, this is not a 'coyote', but it is a 'grey wolf'. <|im_end|>\n<|im_start|>assistant\n<execute>\nnew_concept = 'grey wolf'\nold_concept= 'coyote'\nresponse = add_new_concept(image_id, new_concept, old_concept)\nprint(response)\n</execute> <|im_end|>\n<|im_start|>user\nExecution Output:\n{'reasons': {'descriptions': ['four-legged mammal', 'brown, grey, or black fur', 'pointy ears', 'a bushy tail', 'a long snout', 'sharp teeth'], 'scores': ['0.32', '0.33', '0.32', '0.31', '0.31', '0.31'], 'text': 'This is a 'coyote' because: \n'four-legged mammal': 0.32\n'brown, grey, or black fur': 0.33\n'pointy ears': 0.32\n'a bushy tail': 0.31\n'a long snout': 0.31\n'sharp teeth': 0.31'}, 'type': 'ecole-json-reason'}\n\n<|im_end|>\n<|im_start|>assistant\nThe explanation for the image is:\n\nThis is a 'coyote' because:\n - It is a 'four-legged mammal': 0.32\n - It has 'brown, grey, or black fur': 0.33\n - It has 'pointy ears': 0.32\n - It has 'a bushy tail': 0.31\n - It has 'a long snout': 0.31\n - It has 'sharp teeth': 0.31\n\nThis is the reasoning behind the classification of the image as a 'coyote'. <|im_end|>\n<|im_start|>user\n<image>\n{\n    'id': '657eafdeef3d6763710dffa2',\n     'url': '/images/657eafdeef3d6763710dffa2'\n}{\n    'id': '657eafdeef3d6763710dffb3',\n     'url': '/images/657eafdeef3d6763710dffb3'\n }\n</image>\nWhat is the objects inside those images? <|im_end|>\n<|im_start|>assistant\n<execute>\nimage_id_1 = '657eafdeef3d6763710dffa2'\nimage_id_2 = '657eafdeef3d6763710dffb3'\n[what_is_it(image_id_1), what_is_it(image_id_2)]\n</execute> <|im_end|>\n<|im_start|>user\nExecution Output:\n[{'type': 'ecole-message', 'data': 'This is a 'cat''}, {'type': 'ecole-message', 'data': 'This is a 'dog''}] <|im_end|>\n<|im_start|>assistant\nThere is a 'cat' inside the first image, whereas there is a 'dog' inside the second image. <|im_end|>\n <|im_start|>user",
      "parameters": {
        "temperature": 0.1,
        "top_p": 0.95,
        "truncate": 31744,
        "max_new_tokens": 1024,
        "stop": ["<|im_start|>", "<|im_end|>", "</execute>"]
      }     
  }
]`
OLD_MODELS=`[]`# any removed models, `{ name: string, displayName?: string, id?: string }`
TASK_MODEL='' # name of the model used for tasks such as summarizing title, creating query, etc.

PUBLIC_ORIGIN=#https://huggingface.co
PUBLIC_SHARE_PREFIX=#https://hf.co/chat
PUBLIC_GOOGLE_ANALYTICS_ID=#G-XXXXXXXX / Leave empty to disable
PUBLIC_ANNOUNCEMENT_BANNERS=`[
]`

PARQUET_EXPORT_DATASET=
PARQUET_EXPORT_HF_TOKEN=
PARQUET_EXPORT_SECRET=

RATE_LIMIT= # requests per minute
MESSAGES_BEFORE_LOGIN=# how many messages a user can send in a conversation before having to login. set to 0 to force login right away

PUBLIC_APP_NAME=ECOLE # name used as title throughout the app
PUBLIC_APP_ASSETS=chatui # used to find logos & favicons in static/$PUBLIC_APP_ASSETS
PUBLIC_APP_COLOR=blue # can be any of tailwind colors: https://tailwindcss.com/docs/customizing-colors#default-color-palette
PUBLIC_APP_DESCRIPTION=# description used throughout the app (if not set, a default one will be used)
PUBLIC_APP_DATA_SHARING=#set to 1 to enable options & text regarding data sharing
PUBLIC_APP_DISCLAIMER=0#set to 1 to show a disclaimer on login page
LLM_SUMMERIZATION=true

# PUBLIC_APP_NAME=HuggingChat
# PUBLIC_APP_ASSETS=huggingchat
# PUBLIC_APP_COLOR=yellow
PUBLIC_APP_DESCRIPTION=""
# PUBLIC_APP_DATA_SHARING=1
# PUBLIC_APP_DISCLAIMER=1